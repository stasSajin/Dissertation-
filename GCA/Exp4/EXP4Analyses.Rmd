---
title: "Experiment 4"
author: "Stas Sajin"
date: "October 10, 2015"
output: html_document
---

#Synopsis
The aim of this document is to provide a summary of all the analyses used in writing up the results for Experiment 4 in the dissertation document.The analyses are brokwn down into several parts:

1. Data Cleaning and Pre-processing

2. Exploratory Analyses

3. Growth Curve Analysis

4. Graphs


####*Libraries*
Load the required libraries 
```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(lme4)
library(lmerTest)
library(dplyr)
library(parallel)
library(doParallel)
library(tidyr)
library(gridExtra)
library(caret)
library(multcomp)
```

####*Data Loading*
Not all colums in the results file are required. Only the following colums will be loaded in R, so as to preserve memory space. The colums are: RECORDING_SESSION_LABEL, IA_FIRST_RUN_START_TIME, IA_FIRST_RUN_START_TIME, IA_FIRST_RUN_END_TIME, IA_SECOND_RUN_START_TIME, IA_SECOND_RUN_END_TIME, IA_THIRD_RUN_START_TIME, IA_THIRD_RUN_END_TIME, IA_LABEL, RESPONSE_ACC, IA_DWELL_TIME, TRIAL_INDEX, RESPONSE_RT,trialtype,target,Bilingual, list, colorcompetitor, colorofblock

```{r}
EyeData <- read.csv("Exp4DriftCorrectedVersion2.csv", na.strings = c("."))[,c('RECORDING_SESSION_LABEL', 'IA_FIRST_RUN_START_TIME', 'IA_FIRST_RUN_START_TIME', 'IA_FIRST_RUN_END_TIME', 'IA_SECOND_RUN_START_TIME', 'IA_SECOND_RUN_END_TIME', 'IA_THIRD_RUN_START_TIME', 'IA_THIRD_RUN_END_TIME', 'IA_LABEL', 'RESPONSE_ACC', 'IA_DWELL_TIME', 'TRIAL_INDEX', 'target', 'RESPONSE_RT', 'trialtype','Bilingual','list','colorcompetitor', 'colorofblock')]
#change all the NAs to zeros
EyeData[is.na(EyeData)] <- 0
#check the dimentions of the dataset
dim(EyeData)
#rename the RECORDING_SESSION_LABEL into Subject
colnames(EyeData)[1] <- "Subject"
#examine colum names
names(EyeData)
```

Examine the structure of the dataframe
```{r}
str(EyeData)
```
Everything looks fine.

##*Data Cleaning and PreProcessing*
Remove practice trials
```{r}
EyeData <- subset(EyeData, EyeData$trialtype!="Practice")
```

Check subject accuracy and RT for the selection of the word responses
```{r}
SubjectAccuracy<- EyeData %>% group_by(Subject) %>% 
    summarise(MeanAccuracy=mean(RESPONSE_ACC)) %>% arrange(MeanAccuracy)
SubjectAccuracy
mean(SubjectAccuracy$MeanAccuracy)
SubjectRT<- EyeData %>% group_by(Subject) %>% 
    summarise(MeanRT=mean(RESPONSE_RT)) %>% arrange(MeanRT)
SubjectRT
mean(SubjectRT$MeanRT)

#the average RT was 1716.362-200ms since the RT is measured from the onset of the spoken target.

#look at the accyracy for each colorcondition
EyeData %>% group_by(trialtype, colorcompetitor) %>% 
    summarise(MeanAccuracy=mean(RESPONSE_ACC)) %>% arrange(MeanAccuracy)
```

Subject accuracy is fairly high, though each subjects seems to have between 1-5 inaccurate responses. Allmost all the subjects have 100% accuracy. Only 2 participants have a few incorrect trials.

Remove trials with incorrect accuracy
```{r}
CorrectEyeData<-EyeData %>% filter(RESPONSE_ACC==1)

#examine trials by congruency
CongruencyRT<- CorrectEyeData %>% filter(trialtype=="Experimental") %>% group_by(colorcompetitor) %>% 
    summarise(MeanRT=mean(RESPONSE_RT)) %>% arrange(MeanRT)

#LMER model for button pressing 
Experimental<- CorrectEyeData %>% filter(trialtype=="Experimental")
RTLmer<-lmer(RESPONSE_RT~colorcompetitor+(1+colorcompetitor|Subject)+(1|target), data=Experimental)
summary(RTLmer)
summary(glht(RTLmer, mcp(colorcompetitor="Tukey")))
```

Set up the start time and the end time dummy-coding of each run in the interest area. This creates colums where if a fixation exists during a run, then the run gets 1 (exists) or if there is no fixation in the IA during a run, then the coding is 0 (not exist)
```{r}
CorrectEyeData$Fststart <- ifelse(as.numeric(as.character(CorrectEyeData$IA_FIRST_RUN_START_TIME)) > 0, as.numeric(as.character(CorrectEyeData$IA_FIRST_RUN_START_TIME)), 0) 
CorrectEyeData$Fstend <- ifelse(as.numeric(as.character(CorrectEyeData$IA_FIRST_RUN_START_TIME)) > 0, as.numeric(as.character(CorrectEyeData$IA_FIRST_RUN_END_TIME)), 0)
CorrectEyeData$Secstart <- ifelse(as.numeric(as.character(CorrectEyeData$IA_SECOND_RUN_START_TIME)) > 0, as.numeric(as.character(CorrectEyeData$IA_SECOND_RUN_START_TIME)), 0)
CorrectEyeData$Secend <- ifelse(as.numeric(as.character(CorrectEyeData$IA_SECOND_RUN_START_TIME)) > 0, as.numeric(as.character(CorrectEyeData$IA_SECOND_RUN_END_TIME)), 0)
CorrectEyeData$Thirdstart <- ifelse(as.numeric(as.character(CorrectEyeData$IA_THIRD_RUN_START_TIME)) > 0, as.numeric(as.character(CorrectEyeData$IA_THIRD_RUN_START_TIME)), 0)
CorrectEyeData$Thirdend <- ifelse(as.numeric(as.character(CorrectEyeData$IA_THIRD_RUN_START_TIME)) > 0, as.numeric(as.character(CorrectEyeData$IA_THIRD_RUN_END_TIME)), 0)
```

Generate time bins from time 0 to time 6000 in 25 ms bins and assign them to the dataset
```{r}
time <- seq(0, 6000, by=25)
tmatrix <- matrix(nrow=nrow(CorrectEyeData), ncol=length(time))
dim(tmatrix)
```

Generate time vectors for each row and column for first, second, and third pass viewings 
so that viewing periods receive a viewing probability value of 1 

```{r, warning=FALSE, cache=TRUE}
registerDoParallel(3)
for(i in 1:nrow(tmatrix)) {
for(j in 1:length(time)) {

tmatrix[i,j] <-  ifelse(CorrectEyeData$Fststart[i] < time[j] & 
                CorrectEyeData$Fstend[i] > time[j] |CorrectEyeData$Secstart[i] <
                time[j] & CorrectEyeData$Secend[i] > time[j] | CorrectEyeData$Thirdstart[i] 
                < time[j] & CorrectEyeData$Thirdend[i]>time[j], 1,0)
} 
}
```

Combine the CleanEyeData with the time matrix
```{r}
CleanData <- cbind(CorrectEyeData, data.frame(tmatrix))
```

Assign time values to time bin columns
```{r}
colnames(CleanData)[26:266] <- seq(0, 6000, by=25)
```

Subset the dataset with only the necessary colums and remove anything in the memory that might be a memory hog
```{r}
CleanData <- CleanData[, -c(2:8,10,11,20:25)]
```

Put the data in long-format and then calculate the proportion for each time bin for each subject
```{r}
CleanData<- CleanData %>% gather(time,value,11:251)
#find proportion for each interest area for each subject
CleanDataProb<-CleanData %>% group_by(Subject,colorcompetitor,IA_LABEL,trialtype,time) %>%
    summarise(Prob=mean(value))
CleanDataProb$time<-as.numeric(as.character(CleanDataProb$time))
```


##**Exploratory Graphs**
The aim of this section is to explore pattern in the data without commiting to any formal modeling. Several questions will be addressed thoguh exploratory graphs

1. In trials in which the color of the target is congruent with the color of the block, can we observe competition effects? Are competition effects enhanced when the competitor is in the same color as the target? 

2. Is having bilinguals in this experiment an issue? There were 4 subjects who reported being bilingual, though they spoke English at home. 

```{r}
#subset the required dataset by averaging over subjects; find the usual
OnlySubjects<-CleanData %>% group_by(IA_LABEL,trialtype,time,colorcompetitor) %>%
    summarise(n=n(),Prob=mean(value),sd=sd(value)) %>%
    mutate(se=sd/sqrt(n), LCI=Prob+qnorm(0.025)*se, UCI=Prob+qnorm(0.975)*se)
OnlySubjects$time<-as.numeric(as.character(OnlySubjects$time))

levels(OnlySubjects$IA_LABEL) <- list(Target="TARGET_IA ", Competitor="COMPET_IA ", Distractor1="UNREL1_IA ", Distractor2="UNREL2_IA ")


levels(OnlySubjects$colorcompetitor) <- list(Congruous="Congruous", `Incongruous Competitor`="IncongruousComp", `Incongruous Distr.`="IncongruousDistr")

colnames(OnlySubjects)[1]<-"Interest Area"

```

```{r, warning=FALSE, message=FALSE, cache=TRUE}
#plot for time 0 to time 6000
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
g1<-ggplot(OnlySubjects, aes(x=time, y=Prob, shape=`Interest Area`, color=`Interest Area`)) +
    geom_point(size=3) +
    geom_errorbar(aes(ymin=LCI, ymax=UCI)) + facet_grid(colorcompetitor~trialtype)+
    scale_x_continuous(limits = c(0, 6000))+
    geom_vline(xintercept = 2000+675)+
    geom_vline(xintercept = 5200)+theme_bw()+
    scale_fill_manual(values=cbPalette)+scale_colour_manual(values=cbPalette)+
    scale_x_continuous(name="time from onset of fixation cross(ms)")+
    scale_y_continuous(name="Fixation Proportion")+
    theme(legend.text = element_text(size = 12))+
    theme(legend.title = element_text(size=12, face="bold"))+
    theme(axis.title.x = element_text(face="bold", size=15), axis.text.x  = element_text(size=10))+
    theme(axis.title.y = element_text(face="bold", size=15), axis.text.y  = element_text(size=10))+
    theme(strip.text.x = element_text(size=12, face="bold"))+
    theme(strip.text.y = element_text(size=12, face="bold"))+
    theme(plot.title = element_text(lineheight=.8, face="bold", size=14))
    
#plot from time 1000(fixation cross)+1000(word display with no sound)+682(click on duration).
OnlySubjects$time2<-OnlySubjects$time-2675
g2<-ggplot(OnlySubjects, aes(x=time2, y=Prob, shape=`Interest Area`, color=`Interest Area`)) +
    geom_point(size=3) +
    geom_errorbar(aes(ymin=LCI, ymax=UCI)) + facet_grid(colorcompetitor~trialtype)+
    scale_x_continuous(breaks=seq(0,2400,150), limits=c(0,2400), name="time from display onset (ms)")+
    theme_bw()+
    scale_fill_manual(values=cbPalette)+scale_colour_manual(values=cbPalette)+
    scale_y_continuous(name="Fixation Proportion")+
    theme(axis.title.x = element_text(face="bold", size=15), axis.text.x  = element_text(size=12, angle=90, vjust=0.5))+
    theme(axis.title.y = element_text(face="bold", size=15), axis.text.y  = element_text(size=12))+
    theme(plot.title = element_text(lineheight=.8, face="bold", size=14))+
    theme(strip.text.x = element_text(size=12, face="bold"))+
    theme(strip.text.y = element_text(size=12, face="bold"))

png(filename="plot1Experiment4.png",width=900,height=524)
g1
dev.off()

png(filename="plot2Experiment4.png",width=900,height=524)
g2
dev.off()



```  
    
These graphs indicate that there is no competition going on when the color of the target matches the color of the square and that competition is increased when the color of the competitor matches the color of the target. Since later analyses will examine 1)Target (Congruous) vs Target (Incongruous COmpetitor) vs Target (Incongruous Distractor)  and 2)
Competitor(Congruous) vs COmpetitor (Incongruous COmpetitor) vs. Competitor (Incongruous Distractor), I make the plots of these comparisons here.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
colnames(OnlySubjects)[1]<-"InterestArea"
colnames(OnlySubjects)[4]<-"Congruence"
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
g3<-ggplot(OnlySubjects, aes(x=time2, y=Prob, shape=Congruence, color=Congruence)) +
    geom_point(size=3) +
    geom_errorbar(aes(ymin=LCI, ymax=UCI)) + facet_grid(InterestArea~trialtype)+
    theme_bw()+
    scale_fill_manual(values=cbPalette, name="Congruence Condition")+
    scale_colour_manual(values=cbPalette)+
    scale_y_continuous(name="Fixation Proportion")+
    theme(legend.text = element_text(size = 12))+
    theme(legend.title = element_text(size=12, face="bold"))+
    theme(axis.title.x = element_text(face="bold", size=15), axis.text.x  = element_text(size=12, angle=90, vjust=0.5))+
    theme(axis.title.y = element_text(face="bold", size=15), axis.text.y  = element_text(size=12))+
    theme(strip.text.x = element_text(size=12, face="bold"))+
    theme(strip.text.y = element_text(size=12, face="bold"))+
    theme(plot.title = element_text(lineheight=.8, face="bold", size=14))+
    scale_x_continuous(limits = c(0, 2300), breaks=seq(0,2400,150), name="time from display onset (ms)")

png(filename="plot3Experiment4.png",width=900,height=524)
g3
dev.off()
```

The graph below will examine the data from four bilinguals
    
```{r, warning=FALSE, message=FALSE, cache=TRUE}
OnlySubjects<-CleanData %>% group_by(IA_LABEL,trialtype,time,colorcompetitor,Bilingual) %>%
    summarise(n=n(),Prob=mean(value),sd=sd(value)) %>%
    mutate(se=sd/sqrt(n), LCI=Prob+qnorm(0.025)*se, UCI=Prob+qnorm(0.975)*se)
OnlySubjects$time<-as.numeric(as.character(OnlySubjects$time))


ggplot(OnlySubjects, aes(x=time, y=Prob, shape=IA_LABEL, color=colorcompetitor)) +
    geom_point() +
    geom_errorbar(aes(ymin=LCI, ymax=UCI)) + facet_grid(Bilingual~trialtype)+
    scale_x_continuous(limits = c(2000+675, 5200))+
    theme_bw()
```

There doesn't seem to be any substantial differences between bilinguals and natives. The data patterns look the same, though the bilingual data has substantially more variability in it because there are only 4 subjects, and their responses are distributed acorss 9 conditions (3-trialtype x 3 congruency)



##**Growth Curve Analyses**
The following analyses will examine
1. the proportion fixations in Experimental trials between the Target (Congruent) vs Target (Incongruent Competitor) vs Target (Incongruent Distractor)
2. the proportion fixations in Experimental trials between the Competitor (Congruent) vs Competitor (Incongruent Competitor) vs Competitor (Incongruent Distractor)


###*Target vs. Competitor*
First, the required subset for the `1st` analysis is created
```{r}
model<-CleanDataProb %>% 
    filter(trialtype=="Experimental", IA_LABEL=="TARGET_IA ", colorcompetitor!="Practice", time>=2000+675 & time <=5200)
model$IA_LABEL<-as.factor(as.character(model$IA_LABEL))
model$colorcompetitor<-as.factor(as.character(model$colorcompetitor))

#Generate time polynomials up to quadratic poly.
t25 <- data.frame(poly(unique(model$time),4))
t25$time <- seq(2000+675, 5200, by=25)

#add polynomials to data frame
model <- merge(model , t25, by="time")
head(model)
str(model)
```

The following models are run, with increasing complexity:

Model 1: A base model with no polynomials and only with the fixed effect IA_LABEL and random effect of subject intercept and nested condition with subject intercept.

Model 2: Base model with the addition of the linear polynomial for the fixed and random effect structure

Model 3. Model above+the addition of the quadratic polynomial in the fixed effect and random effect structure

Model 4. Model 3+addition of the third poly in the fixed and random effect structure. Model fails to converge

Model 5. Model 4+addition of the third poly in the fixed and in the random structure(subject only); converged

Model 6. Model 4+addition of the third poly in the fixed and random; fails to converge structure(subject:condition only)
```{r TargetVSTarget,warning=FALSE, message=FALSE, cache=TRUE}
#relevel the colorcompetitor variable, so that multiple comparisons of the factor variable are compared as follows
# target (congruent) vs target (incongruent comp)
# target (incongruent dist) vs target (incongruent distr)
model$colorcompetitorReleveled <- relevel(model$colorcompetitor, "IncongruousComp")

registerDoParallel(3)
#base model; Model1
Model1<-lmer(Prob ~ colorcompetitorReleveled + 
                 (1| Subject) + 
                 (1| Subject:colorcompetitorReleveled),
               data=model, REML=T)
summary(Model1)
#linear; model 2
Model2<-lmer(Prob ~ colorcompetitorReleveled*X1 + 
                 (1+X1| Subject) + 
                 (1+X1| Subject:colorcompetitorReleveled),
               data=model, REML=T)
summary(Model2)
#linea+quadratic; model 3
Model3<-lmer(Prob ~ colorcompetitorReleveled*(X1+X2) + 
                 (1+X1+X2| Subject) + 
                 (1+X1+X2| Subject:colorcompetitorReleveled),
               data=model, REML=T)
summary(Model3)
#Model 4
Model4<-lmer(Prob ~ colorcompetitorReleveled*(X1+X2+X3) + 
                 (1+X1+X2+X3| Subject) + 
                 (1+X1+X2+X3| Subject:colorcompetitorReleveled),
               data=model, REML=T)
#model 4 did not converge
#Model 5
Model5<-lmer(Prob ~ colorcompetitorReleveled*(X1+X2+X3) + 
                 (1+X1+X2+X3| Subject) + 
                 (1+X1+X2| Subject:colorcompetitorReleveled),
               data=model, REML=T)
#model 5 converged
#Model 6
Model6<-lmer(Prob ~ colorcompetitorReleveled*(X1+X2+X3) + 
                 (1+X1+X2| Subject) + 
                 (1+X1+X2+X3| Subject:colorcompetitorReleveled),
               data=model, REML=T)
#model 6 failed to converge
#compare the models based on ANOVA test
anova(Model1,Model2,Model3,Model4,Model5,Model6)
#compare the models based on R^2 based on a paper from Xu (http://onlinelibrary.wiley.com/doi/10.1002/sim.1572/abstract)
1-var(residuals(Model1))/(var(model.response(model.frame(Model1))))
1-var(residuals(Model2))/(var(model.response(model.frame(Model2))))
1-var(residuals(Model3))/(var(model.response(model.frame(Model3))))
1-var(residuals(Model4))/(var(model.response(model.frame(Model4))))
1-var(residuals(Model5))/(var(model.response(model.frame(Model5))))
1-var(residuals(Model6))/(var(model.response(model.frame(Model6))))

#find the summary for model 5
summary(Model5)
#find the RMSE
sqrt(mean((model$Prob-fitted(Model5))^2))

#attach the fitted values to the dataset model, so that they can be used later for graphing purposes
model$Model1Fitted<-fitted(Model1)
model$Model2Fitted<-fitted(Model2)
model$Model3Fitted<-fitted(Model3)
model$Model4Fitted<-fitted(Model4)
model$Model5Fitted<-fitted(Model5)
model$Model6Fitted<-fitted(Model6)

#check that the values are in the dataset
str(model)
```

Model 5 performs the best, though LMER is not very susceptible to corss-validation, so the addition of more polynomials might be leading to overfitting. More complex models could not be fitted due to convergence issues.


###*Compare LMER with Random Forests*
This portion of the report aims to examine if it is even necessary to have so many polynomials in the fixed effect structure of the LMER model used above. Two random forests models are fitted. The models are cross-validated.

The first models examines the inclusion of all 4 polynomials interacting with the IA_LABEL and the inclusion of Subject ID
```{r, cache=TRUE}
set.seed(123)
modelRF1<-train(Prob~(X1+X2+X3+X4)*colorcompetitorReleveled+Subject, method="rf", data=model,
                trControl = trainControl(method = "cv", number = 5), 
                 ntree = 200, importance = TRUE)
#Examine Model Ouput
modelRF1
#Examine the best model
modelRF1$finalModel
#Examine Variable Importance
modelRF1$finalModel$importance
varImpPlot(modelRF1$finalModel)
#Examine RMSE
RMSERandomForest<-sqrt(mean((model$Prob-modelRF1$finalModel$predicted)^2))

#attach the predicted values to the dataset for later graphing purposes
model$RF1<-modelRF1$finalModel$predicted
```

The linear component seems to be very important. A simpler model is implemented below:

```{r, cache=TRUE}
set.seed(123)
modelRF2<-train(Prob~(X1)*colorcompetitorReleveled+Subject, method="rf", data=model,
                trControl = trainControl(method = "cv", number = 5), 
                 ntree = 200, importance = TRUE)
#Examine Model Ouput
modelRF2
#Examine the best model
modelRF2$finalModel
#Examine Variable Importance
modelRF2$finalModel$importance
varImpPlot(modelRF2$finalModel)
#Examine RMSE
sqrt(mean((model$Prob-modelRF2$finalModel$predicted)^2))

#attach the predicted values to the dataset
model$RF2<-modelRF2$finalModel$predicted
```

Simpler model again performs better


-------------------

###*Competitor (Congruent) vs. Competitor (Incongruent COmp) vs. Competitor (Incongruent Distr.)*

The required subset for the `2nd` analysis is created. This analysis compares competitors for each condition

```{r}
#find proportion for each interest area for each subject
CleanDataProb2<-CleanData %>% group_by(Subject,colorcompetitor,IA_LABEL,trialtype,time) %>%
    summarise(Prob=mean(value))
CleanDataProb2$time<-as.numeric(as.character(CleanDataProb2$time))

modelData<-CleanDataProb2 %>% 
    filter(trialtype=="Experimental", IA_LABEL=="COMPET_IA ", colorcompetitor!="Practice", time>=2000+675 & time <=5200)

modelData$colorcompetitor<-as.factor(as.character(modelData$colorcompetitor))

modelData$colorcompetitorReleveled <- relevel(modelData$colorcompetitor, "IncongruousDistr")

#Generate time polynomials up to quadratic poly.
t25 <- data.frame(poly(unique(modelData$time),4))
t25$time <- seq(2675, 5200, by=25)

#add polynomials to data frame
modelData <- merge(modelData , t25, by="time")
head(modelData)
str(modelData)
```


###*Run Growth Curve Analyses*

The following models are run, with increasing complexity:
Model 1: A base model with no polynomials and only with the fixed effect IA_LABEL and random effect of subject intercept and nested condition with subject intercept.

Model 2: Base model with the addition of the linear polynomial for the fixed and random effect structure; Model Fails to converge

Model 3. Model above+random slope of X1 is removed from subjct:condition; fails to converge

Model 4. Model 2 random slope is removed from subject. fails to converge

Model 5. Model 2 wihtout random slopes; no issues

```{r CompVsComp,warning=FALSE, cache=TRUE}
registerDoParallel(3)
#base model; Model1
Model1<-lmer(Prob ~ colorcompetitorReleveled + 
                 (1| Subject) + 
                 (1| Subject:colorcompetitorReleveled),
               data=modelData, REML=T)
summary(Model1)
#linear; model 2
Model2<-lmer(Prob ~ colorcompetitorReleveled*X1 + 
                 (1+X1| Subject) + 
                 (1+X1| Subject:colorcompetitorReleveled),
               data=modelData, REML=T)
summary(Model2) #model fails to converge
#linea+quadratic; model 3
Model3<-lmer(Prob ~ colorcompetitorReleveled*(X1) + 
                 (1+X1| Subject) + 
                 (1| Subject:colorcompetitorReleveled),
               data=modelData, REML=T)
#model 3 fails to converge
#Model 4
Model4<-lmer(Prob ~ colorcompetitorReleveled*(X1) + 
                 (1| Subject) + 
                 (1+X1| Subject:colorcompetitorReleveled),
               data=modelData, REML=T)
#model 4 fails to converge
#Model 5
Model5<-lmer(Prob ~ colorcompetitorReleveled*(X1) + 
                 (1| Subject) + 
                 (1| Subject:colorcompetitorReleveled),
               data=modelData, REML=T)
#Model 5 converges
#compare the models based on ANOVA test
anova(Model1,Model2,Model3,Model4,Model5)
#compare the models based on R^2 based on a paper from Xu (http://onlinelibrary.wiley.com/doi/10.1002/sim.1572/abstract)
1-var(residuals(Model1))/(var(model.response(model.frame(Model1))))
1-var(residuals(Model2))/(var(model.response(model.frame(Model2))))
1-var(residuals(Model3))/(var(model.response(model.frame(Model3))))
1-var(residuals(Model4))/(var(model.response(model.frame(Model4))))
1-var(residuals(Model5))/(var(model.response(model.frame(Model5))))
#find the summary for model 5
summary(Model5)
#find the RMSE
sqrt(mean((modelData$Prob-fitted(Model5))^2))

#attach the fitted values to the dataset modelData, so that they can be used later for graphing purposes
modelData$Model1Fitted<-fitted(Model1)
modelData$Model2Fitted<-fitted(Model2)
modelData$Model3Fitted<-fitted(Model3)
modelData$Model4Fitted<-fitted(Model4)
modelData$Model5Fitted<-fitted(Model5)
```

Model 5 seems to perform the best, while also converging to a result. Next, I will examine what happens in random forests.


####***Compare LMER with Random Forests
This portion of the report aims to examine if it is even necessary to have so many polynomials in the fixed effect structure of the LMER model used above. Two random forests models are fitted. The models are cross-validated.

The first models examines the inclusion of all 4 polynomials interacting with the IA_LABEL and the inclusion of Subject ID
```{r, cache=TRUE}
set.seed(123)
modelRF1<-train(Prob~(X1+X2+X3+X4)*colorcompetitorReleveled+Subject, method="rf", data=modelData,
                trControl = trainControl(method = "cv", number = 5), 
                 ntree = 200, importance = TRUE)
#Examine Model Ouput
modelRF1
#Examine the best model
modelRF1$finalModel
#Examine Variable Importance
modelRF1$finalModel$importance
varImpPlot(modelRF1$finalModel)
#Examine RMSE
RMSERandomForest<-sqrt(mean((model$Prob-modelRF1$finalModel$predicted)^2))
#attach predicted values
modelData$RF1<-modelRF1$finalModel$predicted
```

The third poly and some of the interactions between poly and conditions seem to be redundant

```{r, cache=TRUE}
set.seed(123)
modelRF2<-train(Prob~(X1)*colorcompetitorReleveled+Subject, method="rf", data=modelData,
                trControl = trainControl(method = "cv", number = 5), 
                 ntree = 200, importance = TRUE)
#Examine Model Ouput
modelRF2
#Examine the best model
modelRF2$finalModel
#Examine Variable Importance
modelRF2$finalModel$importance
varImpPlot(modelRF2$finalModel)
#Examine RMSE
sqrt(mean((modelData$Prob-modelRF2$finalModel$predicted)^2))

#attached predicted values
modelData$RF2<-modelRF2$finalModel$predicted
```

The cross-validated random forest model fewer components performs a bit better than the one that has more components. 

##**Graphs**
Each model used above will be graphed by overlaying it on the real data. First, the plots for the first comparison will be created. For this, I use the model dataset. I put the data in long only format.

```{r, cache=TRUE}
ModelLong<- model %>% gather(Model,Predictions,12:19)
#rename the models so that they are more clear
levels(ModelLong$Model) <- list(`Model 1 Intercept`="Model1Fitted", `Model 2 Linear`="Model2Fitted", `Model 3 Quadratic`="Model3Fitted", `Model 4 Cubic (not converged)`="Model4Fitted", `Model 5 Cubic`="Model5Fitted", `Model 6 Cubic (not converged)`="Model6Fitted", `Random Forest Complex`="RF1",`Random Forest Simple`="RF2")

#rename some of the colums
colnames(ModelLong)[3]<-"Congruence" 
ModelLong[,3]<-factor(ModelLong[,3])
#compress the data(find the average over subjects)
ForGraph1<-ModelLong %>% group_by(Congruence,time, Model) %>%
    summarise(`Fixation Probability`=mean(Prob),Predicted=mean(Predictions))
```

The code chunk below will plot and save the graph

```{r, warning=FALSE, cache=TRUE}
#color palette for color-blind
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
#create plot1 for Experiment 1, which goes in the publication
#first thing to do is to filter out the RF models. 
GCAModels<-ForGraph1 %>% filter(Model!="Random Forest Complex" & Model!="Random Forest Simple")
GCAModels$time2<-GCAModels$time-2675

png(filename="GCAModels.png",width=900,height=524)
ggplot(GCAModels, aes(x=time2, y=`Fixation Probability`, shape=Congruence, color=Congruence)) +
    geom_point(size=3) + geom_line(aes(y=Predicted), size=1)+facet_wrap(~Model, nrow=2, ncol=3)+
    scale_fill_manual(values=cbPalette)+scale_colour_manual(values=cbPalette)+
    ggtitle("Fixation Proportion for Models\n examining Targets for each Congruency condition")+
    theme_bw()+
    scale_x_continuous(name="time from display onset (ms)", breaks=seq(0,2300,150), limits=c(0,2300))+
    scale_y_continuous(limits=c(0,1))+
    theme(legend.text = element_text(size = 12))+
    theme(legend.title = element_text(size=12, face="bold"))+
    theme(axis.title.x = element_text(face="bold", size=15), axis.text.x  = element_text(size=12,angle=90,vjust=.5))+
    theme(axis.title.y = element_text(face="bold", size=15), axis.text.y  = element_text(size=12))+
    theme(strip.text.x = element_text(size=12, face="bold"))+
    theme(plot.title = element_text(lineheight=.8, face="bold", size=14))
dev.off()


#not plot the model fits for the RF models
RFModels<-ForGraph1 %>% filter(Model=="Random Forest Complex" | Model=="Random Forest Simple")
RFModels$time2<-RFModels$time-2675

png(filename="RFModels.png",width=900,height=524)
ggplot(RFModels, aes(x=time2, y=`Fixation Probability`, shape=Congruence, color=Congruence)) +
    geom_point(size=3) + geom_line(aes(y=Predicted), size=1)+facet_wrap(~Model, nrow=1, ncol=2)+
    scale_fill_manual(values=cbPalette)+scale_colour_manual(values=cbPalette)+
    ggtitle("Fixation Proportion for Models\n examining Targets for each Congruency condition")+
    theme_bw()+
    scale_x_continuous(name="time from display onset (ms)", breaks=seq(0,2300,150), limits=c(0,2300))+
    scale_y_continuous(limits=c(0,1))+
    theme(legend.text = element_text(size = 12))+
    theme(legend.title = element_text(size=12, face="bold"))+
    theme(axis.title.x = element_text(face="bold", size=15), axis.text.x  = element_text(size=12,angle=90,vjust=.5))+
    theme(axis.title.y = element_text(face="bold", size=15), axis.text.y  = element_text(size=12))+
    theme(strip.text.x = element_text(size=12, face="bold"))+
    theme(plot.title = element_text(lineheight=.8, face="bold", size=14))
dev.off()
```

Now I will plot the second graph for Competitor vs. average of the two distractors. I will use the same steps as for the graph above


```{r, cache=TRUE}
ModelLong<- modelData %>% gather(Model,Predictions,12:18)
#rename the models so that they are more clear
levels(ModelLong$Model) <- list(`Model 1 Intercept`="Model1Fitted", `Model 2 Linear (not converged)`="Model2Fitted", `Model 3 Linear (not converged)`="Model3Fitted", `Model 4 Linear (not converged)`="Model4Fitted", `Model 5 Linear`="Model5Fitted",`Random Forest Complex`="RF1",`Random Forest Simple`="RF2")

#rename some of the colums
colnames(ModelLong)[3]<-"Congruence"
ModelLong[,3]<-factor(ModelLong[,3])
#compress the data(find the average over subjects)
ForGraph2<-ModelLong %>% group_by(Congruence,time, Model) %>%
    summarise(`Fixation Probability`=mean(Prob),Predicted=mean(Predictions))
```

The code chunk below will plot and save the graph

```{r, warning=FALSE, cache=TRUE}
#color palette for color-blind
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
#create plot1 for Experiment 1, which goes in the publication
GCAModels<-ForGraph2 %>% filter(Model!="Random Forest Complex" & Model!="Random Forest Simple")
GCAModels$time2<-GCAModels$time-2675

png(filename="GCAModels.png",width=1000,height=524)
ggplot(GCAModels, aes(x=time2, y=`Fixation Probability`, shape=Congruence, color=Congruence)) +
    geom_point(size=3) + geom_line(aes(y=Predicted), size=1)+facet_wrap(~Model, nrow=2, ncol=3)+
    scale_fill_manual(values=cbPalette)+scale_colour_manual(values=cbPalette)+
    ggtitle("Fixation Proportion for Models\n examining Competitors for each Congruency condition")+
    theme_bw()+
    scale_x_continuous(name="time from display onset (ms)", breaks=seq(0,2300,150), limits=c(0,2300))+
    scale_y_continuous(limits=c(0,1))+
    theme(legend.text = element_text(size = 12))+
    theme(legend.title = element_text(size=12, face="bold"))+
    theme(axis.title.x = element_text(face="bold", size=15), axis.text.x  = element_text(size=12,angle=90,vjust=.5))+
    theme(axis.title.y = element_text(face="bold", size=15), axis.text.y  = element_text(size=12))+
    theme(strip.text.x = element_text(size=12, face="bold"))+
    theme(plot.title = element_text(lineheight=.8, face="bold", size=14))
dev.off()


#not plot the model fits for the RF models
RFModels<-ForGraph2 %>% filter(Model=="Random Forest Complex" | Model=="Random Forest Simple")
RFModels$time2<-RFModels$time-2675

png(filename="RFModels.png",width=900,height=524)
ggplot(RFModels, aes(x=time2, y=`Fixation Probability`, shape=Congruence, color=Congruence)) +
    geom_point(size=3) + geom_line(aes(y=Predicted), size=1)+facet_wrap(~Model, nrow=1, ncol=2)+
    scale_fill_manual(values=cbPalette)+scale_colour_manual(values=cbPalette)+
    ggtitle("Fixation Proportion for Models\n examining Competitors for each Congruency condition")+
    theme_bw()+
    scale_x_continuous(name="time from display onset (ms)", breaks=seq(0,2300,150), limits=c(0,2300))+
    scale_y_continuous(limits=c(0,1))+
    theme(legend.text = element_text(size = 12))+
    theme(legend.title = element_text(size=12, face="bold"))+
    theme(axis.title.x = element_text(face="bold", size=15), axis.text.x  = element_text(size=12,angle=90,vjust=.5))+
    theme(axis.title.y = element_text(face="bold", size=15), axis.text.y  = element_text(size=12))+
    theme(strip.text.x = element_text(size=12, face="bold"))+
    theme(plot.title = element_text(lineheight=.8, face="bold", size=14))
dev.off()
```


Session Info
```{r}
sessionInfo()
```