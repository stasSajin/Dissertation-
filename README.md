# Dissertation-
This repository includes all the data, analyses, and reproducible code for all five experiments in the dissertation titled: "LANGUAGE-VISION INTERACTIONS IN THE VISUAL WORLD PARADIGM: EXAMINING THE EFFECTS OF DISPLAY AND WORKING MEMORY"

###Abstract
Visual World Paradigm (VWP) is commonly used to study a variety of psycholinguistic topics. Nonetheless, as pointed out in several reviews (Huettig, Mishra, & Olivers, 2012; Huettig, Olivers, & Hartsuiker, 2011a; Salverda, Brown, & Tanenhaus, 2011), using the VWP to study language mediated eye-movements should properly account for both linguistic and non-linguistic cognitive factors. In this dissertation, I review studies from the visual search and visual world literature indicating that linguistic representations can be accessed based on visual information and that working memory plays an important role in guiding visual attention. An additional theme that is examined is whether visual search in the VWP is driven by just-in-time processing or from a sparse visual working memory buffer. Following the review of the literature, five experiments examine (1) the pattern of eye movements to printed word displays during the preview period (Experiment 1) , (2) if competitor effects can be guided by linguistic representations stored in working memory rather than online processing of spoken utterances (Experiment 2), (3) if cognitive load and non-linguistic representations stored in working memory affects language mediated eye-gaze (Experiment 3 & 4), and (4) the influence of display size (Experiment 5). The results of these experiments point that linguistic and non-linguistic representations stored in working memory play an important role in guiding eye-movements in the VWP, suggesting that the paradigm can be adapted for a variety of other cognitive tasks (e.g., semantic priming, memory recognition). Moreover, the results of these experiments will be used to argue for a more complex understanding of what drives eye-movements in the VWP. In particular, I will argue that task parameters could affect the mapping of language to visual referents through either a memory-less visual search or a memory-dependent search.
